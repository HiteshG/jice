{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice Hockey Player Tracking Pipeline - Google Colab\n",
    "\n",
    "Complete setup with all model downloads and verification.\n",
    "\n",
    "## Models Summary\n",
    "\n",
    "| Model | Size | Source | Status |\n",
    "|-------|------|--------|--------|\n",
    "| `hockey_yolo.pt` | ~140 MB | **Your custom** | Manual upload |\n",
    "| `sam2.1_hiera_large.pt` | ~898 MB | Meta AI | Auto-download |\n",
    "| `cutie-base-mega.pth` | ~507 MB | GitHub | Auto-download |\n",
    "| `vitpose-h.pth` | ~1.1 GB | OpenMMLab | Auto-download |\n",
    "| `parseq_hockey.ckpt` | ~100 MB | **Your custom** | Manual upload |\n",
    "| `legibility_resnet34_hockey.pth` | ~85 MB | **Your custom** | Manual upload |\n",
    "| SigLIP | ~400 MB | HuggingFace | Auto-download |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Set Home Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = \"/content\"\n",
    "os.chdir(HOME)\n",
    "print(f\"Working directory: {HOME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Clone Your Codebase from GitHub\n",
    "\n",
    "**EDIT THE URL BELOW** to point to your GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EDIT THIS: Your GitHub repository URL\n",
    "# ============================================\n",
    "GITHUB_REPO = \"https://github.com/YOUR_USERNAME/YOUR_REPO.git\"\n",
    "\n",
    "# Clone the repository\n",
    "!git clone {GITHUB_REPO} {HOME}/cv\n",
    "\n",
    "print(f\"\\nRepository cloned to {HOME}/cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the clone\n",
    "!ls -la {HOME}/cv/unified_pipeline/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Install Core Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core packages\n",
    "!pip install -q ultralytics opencv-python-headless pillow tqdm pyyaml omegaconf hydra-core\n",
    "!pip install -q einops timm scipy scikit-learn pandas matplotlib\n",
    "!pip install -q lap cython_bbox filterpy hdbscan\n",
    "!pip install -q transformers accelerate  # For SigLIP\n",
    "!pip install -q umap-learn  # For dimensionality reduction\n",
    "!pip install -q supervision  # For visualization utilities\n",
    "\n",
    "print(\"\\nCore dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Install & Download SAM2 Real-Time Fork\n",
    "\n",
    "SAM2 for high-quality mask generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone SAM2 real-time fork\n",
    "%cd {HOME}\n",
    "!git clone https://github.com/Gy920/segment-anything-2-real-time.git\n",
    "\n",
    "# Install SAM2\n",
    "%cd {HOME}/segment-anything-2-real-time\n",
    "!pip install -e . -q\n",
    "!python setup.py build_ext --inplace\n",
    "\n",
    "print(\"\\nSAM2 real-time installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SAM2 checkpoint (~898 MB)\n",
    "!mkdir -p {HOME}/segment-anything-2-real-time/checkpoints\n",
    "\n",
    "print(\"Downloading SAM2 checkpoint (898 MB)...\")\n",
    "!wget -q --show-progress -O {HOME}/segment-anything-2-real-time/checkpoints/sam2.1_hiera_large.pt \\\n",
    "    https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
    "\n",
    "# Verify\n",
    "!ls -lh {HOME}/segment-anything-2-real-time/checkpoints/sam2.1_hiera_large.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SAM2 loading\n",
    "%cd {HOME}/segment-anything-2-real-time\n",
    "\n",
    "from sam2.build_sam import build_sam2_camera_predictor\n",
    "\n",
    "SAM2_CHECKPOINT = f\"{HOME}/segment-anything-2-real-time/checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_CONFIG = f\"{HOME}/segment-anything-2-real-time/configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "print(\"Loading SAM2 predictor...\")\n",
    "predictor = build_sam2_camera_predictor(SAM2_CONFIG, SAM2_CHECKPOINT)\n",
    "print(\"SAM2 loaded successfully!\")\n",
    "\n",
    "# Clean up to save memory\n",
    "del predictor\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Install & Download CUTIE\n",
    "\n",
    "CUTIE for temporal mask propagation (handles occlusions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone CUTIE\n",
    "%cd {HOME}\n",
    "!git clone https://github.com/hkchengrex/Cutie.git\n",
    "\n",
    "# Install CUTIE\n",
    "%cd {HOME}/Cutie\n",
    "!pip install -e . -q\n",
    "\n",
    "print(\"\\nCUTIE installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CUTIE weights (~507 MB)\n",
    "!mkdir -p {HOME}/Cutie/weights\n",
    "\n",
    "print(\"Downloading CUTIE weights (507 MB)...\")\n",
    "!wget -q --show-progress -O {HOME}/Cutie/weights/cutie-base-mega.pth \\\n",
    "    https://github.com/hkchengrex/Cutie/releases/download/v1.0/cutie-base-mega.pth\n",
    "\n",
    "# Verify\n",
    "!ls -lh {HOME}/Cutie/weights/cutie-base-mega.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Download ViTPose\n",
    "\n",
    "ViTPose for pose estimation (used in jersey number recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mmpose dependencies\n",
    "!pip install -U openmim -q\n",
    "!mim install mmengine -q\n",
    "!mim install \"mmcv>=2.0.0\" -q\n",
    "!mim install \"mmdet>=3.0.0\" -q\n",
    "!mim install \"mmpose>=1.0.0\" -q\n",
    "\n",
    "print(\"\\nmmpose dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ViTPose-H checkpoint (~1.1 GB)\n",
    "!mkdir -p {HOME}/cv/unified_pipeline/models/vitpose\n",
    "\n",
    "print(\"Downloading ViTPose-H (1.1 GB)...\")\n",
    "!wget -q --show-progress -O {HOME}/cv/unified_pipeline/models/vitpose/vitpose-h.pth \\\n",
    "    https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-huge_8xb64-210e_coco-256x192-e32adcd4_20230314.pth\n",
    "\n",
    "# Verify\n",
    "!ls -lh {HOME}/cv/unified_pipeline/models/vitpose/vitpose-h.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Download & Test SigLIP\n",
    "\n",
    "SigLIP for team classification (auto-downloads from HuggingFace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set HuggingFace token if you have one\n",
    "# Uncomment and add your token if needed\n",
    "\n",
    "# import os\n",
    "# os.environ['HF_TOKEN'] = 'your_huggingface_token_here'\n",
    "\n",
    "# Or login interactively:\n",
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and test SigLIP (~400 MB)\n",
    "print(\"Downloading SigLIP from HuggingFace (400 MB)...\")\n",
    "\n",
    "from transformers import AutoProcessor, SiglipVisionModel\n",
    "\n",
    "SIGLIP_MODEL_PATH = 'google/siglip-base-patch16-224'\n",
    "\n",
    "# This downloads the model on first run\n",
    "siglip_model = SiglipVisionModel.from_pretrained(SIGLIP_MODEL_PATH)\n",
    "siglip_processor = AutoProcessor.from_pretrained(SIGLIP_MODEL_PATH)\n",
    "\n",
    "print(\"\\nSigLIP loaded successfully!\")\n",
    "print(f\"Model: {SIGLIP_MODEL_PATH}\")\n",
    "\n",
    "# Clean up to save memory\n",
    "del siglip_model, siglip_processor\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Create Model Directories for Your Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for your custom models\n",
    "!mkdir -p {HOME}/cv/unified_pipeline/models\n",
    "!mkdir -p {HOME}/cv/unified_pipeline/models/parseq\n",
    "!mkdir -p {HOME}/cv/unified_pipeline/models/legibility\n",
    "\n",
    "print(\"Model directories created:\")\n",
    "print(f\"  {HOME}/cv/unified_pipeline/models/\")\n",
    "print(f\"  {HOME}/cv/unified_pipeline/models/parseq/\")\n",
    "print(f\"  {HOME}/cv/unified_pipeline/models/legibility/\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOW UPLOAD YOUR CUSTOM MODELS (see next cells)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: UPLOAD YOUR CUSTOM MODELS\n",
    "\n",
    "### You need to upload these 3 models:\n",
    "\n",
    "| Model | Upload To |\n",
    "|-------|----------|\n",
    "| `hockey_yolo.pt` | `/content/cv/unified_pipeline/models/hockey_yolo.pt` |\n",
    "| `parseq_hockey.ckpt` | `/content/cv/unified_pipeline/models/parseq/parseq_hockey.ckpt` |\n",
    "| `legibility_resnet34_hockey.pth` | `/content/cv/unified_pipeline/models/legibility/legibility_resnet34_hockey.pth` |\n",
    "\n",
    "Run each cell below to upload each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"UPLOAD 1/3: hockey_yolo.pt\")\n",
    "print(\"=\"*60)\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    dest = f'{HOME}/cv/unified_pipeline/models/hockey_yolo.pt'\n",
    "    shutil.move(filename, dest)\n",
    "    size = os.path.getsize(dest) / 1024**2\n",
    "    print(f\"\\nSaved: {dest}\")\n",
    "    print(f\"Size: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"UPLOAD 2/3: parseq_hockey.ckpt\")\n",
    "print(\"=\"*60)\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    dest = f'{HOME}/cv/unified_pipeline/models/parseq/parseq_hockey.ckpt'\n",
    "    shutil.move(filename, dest)\n",
    "    size = os.path.getsize(dest) / 1024**2\n",
    "    print(f\"\\nSaved: {dest}\")\n",
    "    print(f\"Size: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"UPLOAD 3/3: legibility_resnet34_hockey.pth\")\n",
    "print(\"=\"*60)\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    dest = f'{HOME}/cv/unified_pipeline/models/legibility/legibility_resnet34_hockey.pth'\n",
    "    shutil.move(filename, dest)\n",
    "    size = os.path.getsize(dest) / 1024**2\n",
    "    print(f\"\\nSaved: {dest}\")\n",
    "    print(f\"Size: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: VERIFY ALL MODELS\n",
    "\n",
    "**IMPORTANT:** Run this cell to verify all models are properly set up before proceeding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "models = {\n",
    "    \"YOLO Detection\": f\"{HOME}/cv/unified_pipeline/models/hockey_yolo.pt\",\n",
    "    \"SAM2\": f\"{HOME}/segment-anything-2-real-time/checkpoints/sam2.1_hiera_large.pt\",\n",
    "    \"SAM2 Config\": f\"{HOME}/segment-anything-2-real-time/configs/sam2.1/sam2.1_hiera_l.yaml\",\n",
    "    \"CUTIE\": f\"{HOME}/Cutie/weights/cutie-base-mega.pth\",\n",
    "    \"ViTPose\": f\"{HOME}/cv/unified_pipeline/models/vitpose/vitpose-h.pth\",\n",
    "    \"PARSeq\": f\"{HOME}/cv/unified_pipeline/models/parseq/parseq_hockey.ckpt\",\n",
    "    \"Legibility\": f\"{HOME}/cv/unified_pipeline/models/legibility/legibility_resnet34_hockey.pth\",\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_present = True\n",
    "total_size = 0\n",
    "\n",
    "for name, path in models.items():\n",
    "    exists = os.path.exists(path)\n",
    "    if exists:\n",
    "        size = os.path.getsize(path) / 1024**2\n",
    "        total_size += size\n",
    "        status = f\"OK ({size:.1f} MB)\"\n",
    "    else:\n",
    "        status = \"MISSING!\"\n",
    "        all_present = False\n",
    "    print(f\"{name:20s}: {status}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Total model size: {total_size:.1f} MB ({total_size/1024:.2f} GB)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check SigLIP (cached in HuggingFace)\n",
    "try:\n",
    "    from transformers import SiglipVisionModel\n",
    "    print(\"SigLIP:              OK (cached in HuggingFace)\")\n",
    "except:\n",
    "    print(\"SigLIP:              MISSING!\")\n",
    "    all_present = False\n",
    "\n",
    "print(\"=\"*70)\n",
    "if all_present:\n",
    "    print(\"\\n ALL MODELS READY! You can proceed to run the pipeline.\")\n",
    "else:\n",
    "    print(\"\\n SOME MODELS ARE MISSING! Please upload/download them before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Upload Test Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Upload your test video (MP4):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "VIDEO_PATH = None\n",
    "for filename in uploaded.keys():\n",
    "    VIDEO_PATH = f\"{HOME}/{filename}\"\n",
    "    print(f\"\\nVideo uploaded: {VIDEO_PATH}\")\n",
    "    \n",
    "    # Show video info\n",
    "    import cv2\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "    \n",
    "    print(f\"Resolution: {width}x{height}\")\n",
    "    print(f\"FPS: {fps}\")\n",
    "    print(f\"Frames: {frames}\")\n",
    "    print(f\"Duration: {frames/fps:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 13: Create Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}/cv\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.insert(0, f'{HOME}/cv')\n",
    "sys.path.insert(0, f'{HOME}/segment-anything-2-real-time')\n",
    "sys.path.insert(0, f'{HOME}/Cutie')\n",
    "\n",
    "# Create configuration YAML\n",
    "config_yaml = f\"\"\"\n",
    "detection:\n",
    "  model_path: \"{HOME}/cv/unified_pipeline/models/hockey_yolo.pt\"\n",
    "  imgsz: 1280\n",
    "  player_confidence: 0.4\n",
    "  puck_confidence: 0.2\n",
    "  device: \"cuda\"\n",
    "\n",
    "tracking:\n",
    "  track_thresh: 0.6\n",
    "  track_buffer: 30\n",
    "\n",
    "mask:\n",
    "  sam2_checkpoint: \"{HOME}/segment-anything-2-real-time/checkpoints/sam2.1_hiera_large.pt\"\n",
    "  sam2_config: \"{HOME}/segment-anything-2-real-time/configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "  cutie_checkpoint: \"{HOME}/Cutie/weights/cutie-base-mega.pth\"\n",
    "  max_internal_size: 540\n",
    "\n",
    "jersey:\n",
    "  str_model: \"{HOME}/cv/unified_pipeline/models/parseq/parseq_hockey.ckpt\"\n",
    "  legibility_model: \"{HOME}/cv/unified_pipeline/models/legibility/legibility_resnet34_hockey.pth\"\n",
    "  vitpose_checkpoint: \"{HOME}/cv/unified_pipeline/models/vitpose/vitpose-h.pth\"\n",
    "  lock_threshold: 3.0\n",
    "\n",
    "team:\n",
    "  classifier_type: \"hybrid\"\n",
    "  robust_model_name: \"google/siglip-base-patch16-224\"\n",
    "\n",
    "multipass:\n",
    "  enable_backward_pass: true\n",
    "  enable_interpolation: true\n",
    "\n",
    "device: \"cuda\"\n",
    "verbose: true\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{HOME}/cv/config.yaml', 'w') as f:\n",
    "    f.write(config_yaml)\n",
    "\n",
    "print(f\"Configuration saved to {HOME}/cv/config.yaml\")\n",
    "print(\"\\nConfig contents:\")\n",
    "print(config_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 14: Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tracking pipeline\n",
    "OUTPUT_PATH = f\"{HOME}/output_tracked.mp4\"\n",
    "\n",
    "!python -m unified_pipeline.cli \\\n",
    "    --video \"{VIDEO_PATH}\" \\\n",
    "    --output \"{OUTPUT_PATH}\" \\\n",
    "    --config {HOME}/cv/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 15: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check outputs\n",
    "!ls -lh {HOME}/output_tracked*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tracked video\n",
    "print(\"Downloading tracked video...\")\n",
    "files.download(f'{HOME}/output_tracked.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MOT format results (if generated)\n",
    "mot_path = f'{HOME}/output_tracked.txt'\n",
    "if os.path.exists(mot_path):\n",
    "    print(\"Downloading MOT results...\")\n",
    "    files.download(mot_path)\n",
    "else:\n",
    "    print(\"No MOT file generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional: Preview Video in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first few frames\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_frames(video_path, num_frames=4):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_frames, figsize=(20, 5))\n",
    "    \n",
    "    for i, frame_idx in enumerate([int(total * j / num_frames) for j in range(num_frames)]):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            axes[i].imshow(frame_rgb)\n",
    "            axes[i].set_title(f'Frame {frame_idx}')\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    cap.release()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Tracked video preview:\")\n",
    "show_frames(f'{HOME}/output_tracked.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Alternative: Run Without Jersey/Team (Faster)\n",
    "\n",
    "For quick detection + tracking only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast mode: detection + tracking only\n",
    "!python -m unified_pipeline.cli \\\n",
    "    --video \"{VIDEO_PATH}\" \\\n",
    "    --output {HOME}/output_fast.mp4 \\\n",
    "    --config {HOME}/cv/config.yaml \\\n",
    "    --no-jersey \\\n",
    "    --no-team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Paths Quick Reference\n",
    "\n",
    "| Model | Path |\n",
    "|-------|------|\n",
    "| YOLO Detection | `/content/cv/unified_pipeline/models/hockey_yolo.pt` |\n",
    "| SAM2 Checkpoint | `/content/segment-anything-2-real-time/checkpoints/sam2.1_hiera_large.pt` |\n",
    "| SAM2 Config | `/content/segment-anything-2-real-time/configs/sam2.1/sam2.1_hiera_l.yaml` |\n",
    "| CUTIE | `/content/Cutie/weights/cutie-base-mega.pth` |\n",
    "| ViTPose | `/content/cv/unified_pipeline/models/vitpose/vitpose-h.pth` |\n",
    "| PARSeq | `/content/cv/unified_pipeline/models/parseq/parseq_hockey.ckpt` |\n",
    "| Legibility | `/content/cv/unified_pipeline/models/legibility/legibility_resnet34_hockey.pth` |\n",
    "| SigLIP | `google/siglip-base-patch16-224` (HuggingFace, auto-cached) |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
